---
title: C√≥mo montar una arquitectura 100% Cloud Native para nuestra Inteligencia Artificial con Semantic Kernel y Qdrant
slug: /revistas/numero-56/cloud-native-semantic-kernel
date: 01/06/2023
magazine: 56
author: Rodrigo Liberoff
authorId: rodrigo-liberoff
keywords: ['Cloud Native', 'Semantic Kernel', 'Qdrant', 'AI', 'OpenAI']
featuredImage: ../../../images/numero56/cloud-native-semantic-kernel/image1.png
---


A menos que est√©s viviendo bajo una piedra, o te hayas consagrado a ermita√±o, seguro estar√°s como todos nosotros contantemente inundado de noticias e innovaciones sobre la Inteligencia Artificial. De hecho, algunas personas ya bromean con que cada semana salen nuevos productos o servicios de Inteligencia Artificial (IA) como si fueran frameworks de JavaScript.
Dentro de ese contexto, una de las tecnolog√≠as de IA que m√°s importancia est√° tomando dentro del mundo de quienes nos dedicamos a trabajar con las tecnolog√≠as de Microsoft es Semantic Kernel.
La primera parte puede ser un poco te√≥rica, con muchas explicaciones para dar contexto a lo que vamos a montar. Si quieres ir directo al c√≥digo (en C#), ve al t√≠tulo ‚ÄúLa arquitectura de referencia‚Äù üèÉ
Otra cosa, el c√≥digo que veremos en este art√≠culo lo pod√©is encontrar en mi GitHub aqu√≠ üëâ https://github.com/rliberoff/CompartiMOSS-Demo-June-2023 

**¬øQu√© es Semantic Kernel?**
Semantic Kernel es una librer√≠a de c√≥digo abierto (open source) muy liguera que principalmente se enfoca en facilitar la combinaci√≥n de mensajes e indicaciones ‚Äì los famosos prompts ‚Äì para Inteligencias Artificiales generativas basadas en las tecnolog√≠as de OpenAI, con distribuciones para los lenguajes C#, Python y TypeScript, aunque en cada caso la fecha de salida de caracter√≠sticas es un tanto diferente, siendo C# el lenguaje que m√°s actualizaciones recibe.
Semantic Kernel ha sido dise√±ado para permitir que los desarrolladores integren tecnolog√≠as de IA de manera flexible en sus aplicaciones. Para hacerlo, Semantic Kernel proporciona un conjunto de abstracciones que facilitan la creaci√≥n y administraci√≥n de prompts, memorias, funciones y conectores. 
Gracias a las abstracciones que proporciona Semantic Kernel, se puede usar para organizar las capacidades de IA de casi cualquier proveedor. Por ejemplo, se puede usar Semantic Kernel para orquestar una AI desde OpenAI, Azure Open AI e incluso Hugging Face.
Esta librer√≠a es tan poderosa y significativa dentro del marco de negocio para Inteligencia Artificial que est√° proponiendo Microsoft, que se est√° convirtiendo en el eje fundacional principal en la implementaci√≥n de servicios y aplicativos que conocemos con el apelativo de ¬´Copilots¬ª, o tambi√©n plug-ins potenciados por IA.
Esto quiere decir que gracias a Semantic Kernel y sus capacidades de integraci√≥n de tecnolog√≠as de IA, ser√° muy f√°cil construir nuestras propias aplicaciones de la IA, y distribuirlas dentro de la categor√≠a de ¬´Copilots¬ª. Es m√°s, dos de los ejemplos m√°s emblem√°ticos de esta librer√≠a son el Copilot Chat y el GitHub Repo Q&A Copilot.
Por cierto, Semantic Kernel tiene una excelente y muy activa comunidad en Discord  https://discord.com/invite/VpnfAZkv2a 

**¬øQu√© es la ‚Äúmemoria‚Äù en Semantic Kernel?**

La memoria es uno de los componentes fundamentales de Semantic Kernel, y corresponde al mecanismo para proporcionar un contexto m√°s amplio para las consultas e interacciones de los usuarios de nuestros aplicativos con las Inteligencias Artificiales que los potencian.
Semantic Kernel abstrae la memoria en un concepto que nos permitir√° realizar las siguientes tres acciones independientemente de c√≥mo est√© implementada la memoria:
1.	Pares clave-valor: de toda la vida. Se considera una b√∫squeda convencional porque es una coincidencia uno a uno entre una clave y el valor de una consulta.

2.	Almacenamiento local: cuando se guarda informaci√≥n en un archivo, se puede recuperar a partir de su nombre o cierta metadata. No es muy diferente al anterior, salvo por el hecho de que se almacena la informaci√≥n o bien para preservarla de forma m√°s permanente, o bien porque el contenido es tan grande que es necesario apoyarse en recursos como discos duros para mantenerlos.

3.	B√∫squeda sem√°ntica: el m√°s interesante dentro de las Inteligencias Artificiales. En este caso, la informaci√≥n se representa como un vector muy largo de n√∫meros, conocido como embeddings (prefiero usar el t√©rmino en ingl√©s porque digamos que las traducciones al espa√±ol de ‚Äúincrustaciones‚Äù o ‚Äúempotramientos‚Äù son poco atractivas üòâ). Esto permite ejecutar una b√∫squeda que compara un significado con otro significado dentro de una consulta por lo cercano que est√©n sus representaciones vectoriales num√©ricas, que en s√≠ es una definici√≥n de ‚Äúsem√°ntica‚Äù.

Los embeddings son una forma de representar palabras u otros datos como vectores en un espacio de alta dimensi√≥n. Los vectores son como flechas que tienen una direcci√≥n y una longitud. La idea de ‚Äúalto dimensi√≥n‚Äù hace referencia a que el espacio vectorial tiene muchas dimensiones, m√°s de las que podemos ver o imaginar como humanos. La idea es que palabras o datos similares tengan vectores similares, y palabras o datos diferentes tendr√°n vectores diferentes. Esto nos ayuda a medir qu√© tan relacionados o no est√°n entre s√≠, y tambi√©n a realizar operaciones en ellos, como sumar, restar, multiplicar, etc. para obtener combinaciones sem√°nticas m√°s complejas e interesantes. Los embeddings son √∫tiles para los modelos de IA porque pueden capturar el significado y el contexto de palabras o datos de una manera que pueden entender y procesar.
B√°sicamente, toma una oraci√≥n, un p√°rrafo o una p√°gina completa de texto y luego genera el embedding (el vector) correspondiente. Cuando se realiza una consulta, √©sta tambi√©n se transforma a su representaci√≥n como embedding, y luego se realiza una b√∫squeda a trav√©s de todos los vectores existentes para encontrar los m√°s similares. Esto es parecido a cuando realizamos una consulta en Bing y nos devuelve m√∫ltiples resultados cercanos o parecidos a lo que queremos.
Lo cierto es que el trabajo de crear estos embeddings puede llegar a ser laborioso y con muchos detalles que pueden desembocar en errores o comportamientos esperados en nuestras aplicaciones.
All√≠ es donde entra Semantic Kernel, ayud√°ndonos a configurar los aspectos de la memoria de una forma sencilla para que sea despu√©s la librer√≠a la que se encargue de las transformaciones, recuperaciones e incluso las b√∫squedas sem√°nticas correspondientes. B√°sicamente, nos podemos olvidar de toda la teor√≠a anterior üòÖ
El otro problema es c√≥mo almacenar, indexar, preservar y recuperar los embeddings partiendo del hecho de que son un tipo de dato bastante particular y grande. 
Aqu√≠ es donde entran las bases de datos vectoriales, un tipo espec√≠fico de bases de datos especiales. Las bases de datos vectoriales no son nuevas en el mercado de las bases de datos, existen desde hace varios a√±os y se empleaban principalmente en la disciplina del An√°lisis de Datos o en modelos muy sofisticados y complejos de Inteligencia Artificial basada en ¬´Machine Learning¬ª.
Con la entrada explosiva de los algoritmos de IA generativa ‚Äì como nuestro amigo GPT-3 y GPT-4 de Open AI y Azure que dan potencia a ya ultra famoso ChatGPT ‚Äì los desarrolladores de aplicaciones (como los Copilots) est√°n encontrando la necesidad de contar con este tipo de bases de datos para sus productos y servicios.
En ese √°mbito, la oferta es bastante interesante:
‚Ä¢	Azure Cognitive Search Service
‚Ä¢	Azure Cosmos DB
‚Ä¢	Pinecone
‚Ä¢	Postgres
‚Ä¢	Qdrant
Todas estas bases de datos vectoriales (y m√°s que est√°n en la hoja de ruta) son soportadas por Semantic Kernel, aunque principalmente en C#, aun cuando el plan es dar soporte a Python, TypeScript y Java.

![Imagen 1 - Tabla con el estado actual del desarrollo de conectores en Semantic Kernel para las principales bases de datos vectoriales (cuadro amarillo) a mayo de 2023.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

En Semantic Kernel, la memoria es considerada como un con componente sem√°ntico que manipula texto, lo que implica que, por ahora, Semantic Kernel a nivel de memoria no gestiona contenidos diferentes de texto como podr√≠an ser im√°genes, audios o v√≠deos.
Cuando trabajamos con memorias en Semantic Kernel debemos tener en cuenta otro concepto, el de ‚Äúcolecciones‚Äù.
Una colecci√≥n en memoria es como un caj√≥n donde se almacenan e indexan los embeddings para ser recuperados y procesados durante las b√∫squedas sem√°nticas. 
Como en el caso de las bases de datos relacionales, al utilizar bases de datos vectoriales tenemos que realizar un dise√±o de c√≥mo se almacenar√° la informaci√≥n. En este caso, como ser√° la forma en que se distribuyan los embeddings en las colecciones. Hay much√≠simas estrategias, como por ejemplo segregar la informaci√≥n por su enfoque sem√°ntico, por su estructura f√≠sica o por su funci√≥n.
Por ejemplo, si implementamos una aplicaci√≥n de IA que sirva para manipular el contenido de los libros de una biblioteca, podr√≠amos dise√±ar las colecciones en funci√≥n de los autores, los g√©neros, los a√±os de publicaci√≥n o cualquier otra caracter√≠stica que nos permita dise√±ar y programas a qu√© colecci√≥n enviar un determinado embedding. Esto no quiere decir que un mismo embedding no se pueda enviar a m√°s de una colecci√≥n, eso es algo perfectamente factible, pero como en el caso de las bases de datos relacionales, podr√≠a llegar a ser poco √≥ptimo. Podr√≠amos decir que es el equivalente a una desnormalizaci√≥n de un modelo relacional, y que lo aplicar√≠amos en favor del rendimiento del aplicativo.
Otro elemento importante es entender c√≥mo se generan los embeddings y c√≥mo son comparados entre s√≠ para buscar coincidencias sem√°nticas. Actualmente existen much√≠simos algoritmos de generaci√≥n de embeddings, y cada algoritmo favorece en mayor o menor medida a un algoritmo de b√∫squeda y comparaci√≥n. Por ejemplo, el modelo m√°s utilizado para generar embeddings de OpenAI llamado 
¬´text-embedding-ada-002¬ª favorece los algoritmos de comparaci√≥n basados en la funci√≥n de similitud por coseno.
Al elegir una base de datos vectorial debemos tener en cuenta c√≥mo trata y procesa los embeddings para asegurarnos que emplea la funci√≥n de similitud y b√∫squeda sem√°ntica que mejor se adapta a nuestros requerimientos t√©cnicos.
En Semantic Kernel, al recuperar elementos de la memoria podemos indicar cu√°l es el nivel o √≠ndice de relevancia m√≠nimo que esperamos de los resultados al realizar b√∫squedas sem√°nticas. El √≠ndice de relevancia es un valor entre cero y uno (b√°sicamente un porcentaje) que determina cu√°n relevante debe ser el resultado de la b√∫squeda en comparaci√≥n sem√°ntica con el t√©rmino o consulta proporcionado para la misma, correspondiendo el valor uno, a una coincidencia exacta.
A parte del soporte para las bases de datos antes mencionadas, Semantic Kernel ofrece en su librer√≠a el tipo VolatileMemoryStore, el cual se puede considerar como un almacenamiento temporal en memoria que no escribe en el disco y s√≥lo est√° disponible durante la ejecuci√≥n de la aplicaci√≥n. Es s√∫per pr√°ctico para pruebas o desarrollos en local.

**¬øPor qu√© Qdrant como base de datos vectorial?**
 
Lo cierto es que Azure Cosmos DB o Azure Cognitive Search Service pueden ser opciones m√°s interesantes dentro del mundo productivo y empresarial por su sello de garant√≠a Microsoft que asegura la calidad, el nivel de servicio (SLA) y los aspectos de seguridad y privacidad.
Lamentablemente, para el momento hist√≥rico de escribir este art√≠culo, estos servicios como base de datos vectoriales est√°n a√∫n en ¬´Preview¬ª, y por tanto no ser√≠an muy recomendables para usar en entornos productivos (por ahora üòâ).
El producto de base de datos vectoriales de Qdrant por otro lado, est√° muy depurado. Tiene tiempo en el mercado, ofrecen una versi√≥n desplegable en Azure y otra que podemos desplegar individualmente por nuestra parte a trav√©s de contenedores y que se ofrece en modalidad de c√≥digo abierto (open source).
La gente detr√°s de Semantic Kernel tienen una excelente relaci√≥n profesional y de negocios con la gente de Qdrant, lo que podr√≠amos asumir garantiza una conectividad y soporte muy interesantes, algo que podemos evidenciar en este art√≠culo del blog de Semantic Kernel.
Otra ventaja de Qdrant es que es muy r√°pida, muy simple y sencilla de administrar, raz√≥n por la cual es la que elegimos para este art√≠culo (y en general para los proyectos que estoy realizando).
La arquitectura de referencia
La arquitectura que montaremos es 100% ¬´Cloud Native¬ª, es decir, que usaremos exclusivamente recursos y servicios que s√≥lo podemos obtener a partir del uso de una nube p√∫blica, en nuestro caso, Azure üòé
 
![Imagen 2.- Arquitectura de referencia de la infraestructura 100% ¬´Cloud Native¬ª que montaremos.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Lo primero es contar con los servicios y recursos de soporte; para los cuales en este caso necesitaremos:
‚Ä¢	Un Azure Log Analytics Workspace donde enviar las trazas y m√©tricas de los servicios de Azure, la base de datos Qdrant y nuestra aplicaci√≥n con Semantic Kernel.
‚Ä¢	Un Azure Application Insights para explotar las trazas y m√©tricas recabadas en el Log Analytics Workspace.
‚Ä¢	Un Azure Key Vault para los secretos y cadenas de conexiones.
‚Ä¢	Un Azure App Configuration que nos servir√° como el repositorio de los par√°metros de nuestra aplicaci√≥n. Este es un recurso de Azure especialmente muy √∫til para aplicaciones dockerizadas.
‚Ä¢	Un Azure Container Registry donde conservar las im√°genes de nuestra aplicaci√≥n.
Estos servicios y recursos podemos aprovisionarlos en nuestra subscripci√≥n de Azure como mejor nos convenga, usando PowerShell, Azure CLI o directamente desde el Portal de Azure.
Esta arquitectura asume que ten√©is aprovisionado un recurso de Azure OpenAI para la parte que veremos m√°s delante de vinculaci√≥n con el Semantic Kernel. Tambi√©n os servir√° una cuenta directa de OpenAI.
Lo siguiente a aprovisionar es un Azure Container Environment (ACE) donde se alojar√°n las Azure Container Apps (ACA) correspondientes a la base de datos vectorial de Qdrant y nuestra aplicaci√≥n con Semantic Kernel. De momento ‚Äì hasta donde yo s√© ‚Äì no hay un modo de crear un ACE independientemente de un ACA desde el Portal de Azure, por lo cual usaremos el Azure CLI para hacerlo.
Antes de eso, necesitamos averiguar la clave de conexi√≥n de nuestro Azure Log Analytics Workspace para conectarlo con el ACE. Para ello, desde el Azure Portal y dentro de nuestro Log Analytics Workspace, en el men√∫ de la izquierda elegimos ¬´Agents¬ª y en la pantalla que nos aparece extendemos la secci√≥n ¬´Log Analytics agent instructions¬ª para poder ver los valores del ID del workspace y las claves (keys).
 
![Imagen 3.- Donde encontrar el ID y la clave de nuestro Azure Log Analytics Workspace en el Portal de Azure.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Otra alternativa es usando el Azure CLI:
 
![Imagen 4.- Como obtener el Workspace Id y las claves del Azure Log Analytics Workspace desde Azure CLI para usar al crear un Azure Container Environment.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Con estos datos ya podemos aprovisionar nuestro Azure Container Environment (ACE) mediante el siguiente comando del Azure CLI:
 
![Imagen 5.- Usando el Azure CLI para crear un Azure Container Environment.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

La ejecuci√≥n de este comando puede tomar unos varios minutos, as√≠ que se paciente y espera a que te retorne el resultado que seguro ser√° exitoso.
Una vez tenemos nuestro ACE, lo siguiente es crear los ACAs. Esto lo haremos desde el Azure Portal directamente (aunque tambi√©n se puede desde el Azure CLI).
El primer ACA que crearemos es el de Qdrant, para ello desde el Portal de Azure seguimos los pasos que te muestro en las siguientes im√°genes:
 
![Imagen 6.- Donde crear aplicaciones dentro de un Azure Container Environment espec√≠fico.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

 
![Imagen 7.- Configurando los datos b√°sicos de la Azure Container App para Qdrant.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Al configurar el contenedor a desplegar es muy importante seguir los siguientes pasos:
1.	Desmarcar la caja (checkbox) de ¬´Use quickstart image¬ª.
2.	Elegir como origen de la imagen (image source) la opci√≥n de ¬´Docker Hub or other registries¬ª y la opci√≥n de ¬´Public¬ª.
3.	El servidor de im√°genes que usaremos es docker.io.
4.	El nombre y etiqueta (tag) de la imagen de Qdrant que usaremos es qdrant/qdrant:latest.
5.	El resto de las opciones las dejaremos como est√°n.
 
![Imagen 8.- Configuraci√≥n de la imagen de Qdrant para el Azure Container App.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Lo siguiente es configurar c√≥mo ser√° el ingreso al contenedor (ingress) para lo cual tambi√©n es muy importante seguir estos pasos:
1.	Habilitar el ingreso (ingress) marcando la caja (checkbox) de ¬´Enabled¬ª.
2.	En el tipo de tr√°fico permitido, elegir la opci√≥n de ¬´Limited to Container Apps Environment¬ª. Esta es la opci√≥n m√°s importante en esta configuraci√≥n pues la base de datos de c√≥digo abierto (open source) de Qdrant no tiene seguridad alguna en cuanto a control de acceso. Es decir, carece de un usuario y contrase√±a, por lo cual es fundamental que sea absolutamente imposible llegar a ella desde el Internet, redes p√∫blicas o cualquier otro sistema que no est√© bajo nuestro control dentro del mismo Azure Container Environment (ACE).
3.	Como puerto de entrada o ¬´Target port¬ª debemos poner 6333 que es el puerto por defecto por el cual escucha Qdrant las peticiones.
 
![Imagen 9.- Configuraci√≥n del ingreso (ingress)  para el contenedor de Qdrant. N√≥tese que el acceso es limitado s√≥lo a otras aplicaciones desplegadas dentro del mismo Azure Container Environment, lo cual se hace por temas de seguridad.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

El resto de las opciones las podemos dejarla por defecto, y simplemente tenemos que esperar a que se aprovisione nuestro ACA con Qdrant.
Mientras tanto, podemos ir creando el volumen en el cual preservar el estado de la base de datos. Al ser Qdrant una base de datos dockerizada es fundamental contar con un volumen montado y asociado a su ACA para que los archivos y estados de las colecciones que gestionen est√©n ubicadas fuera del contexto del contenedor, tal que, si este se cae, reinicia o sufre alg√∫n corte de servicio, la informaci√≥n no se pierda.
El medio ideal y m√°s f√°cil de configurar es un Azure File Share, ya que es el medio de integraci√≥n de vol√∫menes que trae por defecto los Azure Container Environments (ACE).
Para esto, lo primero es contar con un Azure Storage Account, en el cual crearemos un nuevo contenedor para File Share que llamaremos qdrant. Mi recomendaci√≥n es que us√©is el plan (tier) ¬´Hot¬ª.
 
![Imagen 10.- Creaci√≥n de un Azure File Share para usar como volumen y preservar los datos de Qdrant.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Un punto importante antes de crear la conexi√≥n con nuestro ACE es buscar la clave de acceso del Azure Storage Account para poder realizar la vinculaci√≥n, como muestro en la siguiente pantalla:
 
![Imagen 11.- Donde encontrar las claves del Azure Storage Account, necesarias para crear la vinculaci√≥n del Azure File Share como volumen en el Azure Container Environment.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Ahora s√≠, volvemos a nuestro ACE para agregar el Azure File Share y usarlo como espacio donde montar nuestro volumen para el contenedor de Qdrant. Es importante que el modo de acceso sea de lectura y escritura:
 
![Imagen 12.- Configuraci√≥n del Azure Files en el Azure Container Environment.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Una vez hecho esto, tenemos que modificar la configuraci√≥n del ACA de Qdrant. Para ello, usando el Azure CLI ejecutamos el siguiente comando:
 
![Imagen 13.- Comando del Azure CLI para extraer la configuraci√≥n del contenedor de Qdrant.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)


Este comando nos crear√° un fichero llamado app-qdrant.yaml el cual abriremos con nuestro editor de confianza (Visual Studio Code üòâ) para editarlo:
 
En la imagen quiz√°s se ve muy peque√±o, as√≠ que explicar√© lo que estoy haciendo:
1.	En el fichero YAML, reemplazo la parte que dice volumes: null por:
volumes:
- name: compartimoss-qdrant-azure-file-volume
  storageName: qdrant
  storageType: AzureFile

Este cambio lo que hace es indicar que crearemos un volumen de tipo Azure File de nombre qdrant y que llamaremos compartimoss-qdrant-azure-file-volume.

2.	Agregaremos una configuraci√≥n volumesMount para indicar d√≥nde debe montarse el volumen desde dentro del contenedor de Qdrant:
volumeMounts:
- volumeName: compartimoss-qdrant-azure-file-volume
  mountPath: /qdrant/storage  

Aqu√≠ se vincula el punto de montaje del volumen con el volumen creado en el paso anterior. As√≠, cuando la base de datos Qdrant genere las colecciones en el directorio /qdrant/storage, realmente se estar√°n almacenando en el Azure File Share qdrant que creamos y vinculamos al ACE anteriormente.
Ahora nos queda s√≥lo actualizar el contenedor con el YAML modificado, para lo cual ejecutamos el siguiente comando en Azure CLI:
 
![Imagen 14 - Comando de Azure CLI para actualizar el contenedor de Qdrant con la configuraci√≥n modificada para que tenga un volumen vinculado donde dejar de forma permanente sus datos e informaci√≥n que gestiona.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Y listo, ya tenemos nuestra base de datos vectorial de Qdrant ejecut√°ndose en un ACA con un acceso restringido a s√≥lo el ACE donde vive, para asegurar que nadie podr√° llegar hasta ella desde el exterior.
Ahora vamos con el c√≥digo del Semantic Kernel. Para ello tendremos en este caso una aplicaci√≥n web sencilla con ASP.NET dockerizada. As√≠, lo primero es traernos los paquetes NuGet que necesitaremos:
‚Ä¢	Microsoft.SemanticKernel.Core
‚Ä¢	Microsoft.SemanticKernel.Connectors.Memory.Qdrant
Al trabajar con Qdrant en nuestro c√≥digo, necesitaremos tener en cuenta tres par√°metros:
‚Ä¢	Host  el cual corresponde a la URL donde est√° desplegado Qdrant, y que en nuestro caso ser√° el FQDN del ACA donde lo desplegamos.
‚Ä¢	Port  el puerto por donde escucha solicitudes Qdrant, en nuestro caso ser√° 443, ya que los ACAs reciben todos sus ingresos (ingress) por los puertos est√°ndares de HTTP. Ya ser√° el propio ACA que contiene el Qdrant el que se encargue de pasar las peticiones desde el puerto 443 al puerto 6333.
‚Ä¢	Tama√±o de vectores  Este es un t√©rmino entero (int) que define el tama√±o m√°ximo de los vectores dentro de las colecciones. No es de mayor relevancia, por lo que suelo recomendar el valor por defecto de 1536. Lo importante a saber aqu√≠ es que valores peque√±os para este par√°metro implican que los vectores de un determinado embedding ser√°n m√°s peque√±os y que la informaci√≥n estar√° m√°s segmentada dentro de la colecci√≥n, mientras que valores m√°s grandes ser√°n menos segmentados, pero podr√≠an ser m√°s lentos de procesar en b√∫squedas sem√°nticas. La idea es buscar el valor m√°s conveniente para nuestra aplicaci√≥n. 
Para gestionar estos valores, he creado una clase de opciones que he llamado QdrantOptions y que mapea a una secci√≥n en el appsettings.json o bien a valores dentro del Azure App Configuration:
 
En el caso del propio Semantic Kernel tambi√©n he creado una clase de opciones para gestionar por configuraci√≥n algunos de sus par√°metros. A dicha clase la he llamado SemanticKernelOptions:
 
En esta clase se gestionan las siguientes opciones de configuraci√≥n:
‚Ä¢	CompletionsModel  corresponde al nombre del modelo para completaciones desplegado en nuestra cuenta de OpenAI, donde en mi ejemplo suelo usar text-davinci-003.
‚Ä¢	EmbeddingsModel  corresponde al nombre del modelo para embeddings desplegado en nuestra cuenta de OpenAI y que usar√° el Semantic Kernel en conjunto con la memoria (en este caso la base de datos vectorial de Qdrant) para generar y leer los embeddings de los textos procesados y las consultas realizadas. En mi ejemplo suelo usar text-embedding-ada-002.
‚Ä¢	Endpoint  es la URL al servicio de Azure OpenAI (o de OpenAI per se).
‚Ä¢	Key  es la clave de acceso al servicio de OpenAI.

Como os comentaba en la arquitectura de referencia, como estas aplicaciones est√°n dockerizadas, lo m√°s convenien te es tener estos par√°metros y opciones de configuraci√≥n en un Azure App Configuration:
 
![Imagen 15 - Opciones de configuraci√≥n para Qdrant y Semantic Kernel mantenidas y gestionadas dentro de un Azure App Configuration.](../../../images/numero56/cloud-native-semantic-kernel/image1.png)

Lo siguiente es agregar nuestras clases de opciones al contenedor de dependencias, para lo cual escribimos las siguientes l√≠neas de c√≥digo en nuestro Program.cs:

![](../../../images/numero56/cloud-native-semantic-kernel/image1a.png)
Al agregar Semantic Kernel y Qdrant a nuestro proyecto, debemos hacerlo de la forma correcta para que la Inyecci√≥n de Dependencias funcione a perfecci√≥n. Para ello, en nuestro Program.cs tendremos las siguientes l√≠neas:

![](../../../images/numero56/cloud-native-semantic-kernel/image1b.png)

En las l√≠neas 8 a 12, agregamos una memoria basada en Qdrant como una implementaci√≥n del tipo de interfaz IMemoryStore. Este es el tipo de interface que permite definir conectores con bases y repositorios de datos. En este caso, para Qdrant, es m√°s conveniente que la instancia de IMemoryStore sea un Singleton.
En las l√≠neas 15 a 32 configuramos el Semantic Kernel. As√≠:
‚Ä¢	En las l√≠neas 17 y 18 por inyecci√≥n de dependencias obtenemos los par√°metros de configuraci√≥n para el Semantic Kernel as√≠ como una instancia v√°lida de ILogger para enviar trazas y registros del comportamiento de nuestra aplicaci√≥n al Azure Log Analytics Workspace a trav√©s de Azure Applications Insights, el cual estoy asumiendo habr√©is configurado previamente en el Program.cs üòâ (si no, ten√©is el c√≥digo en GitHub)
‚Ä¢	En la l√≠nea 20 se crea la instancia del Semantic Kernel, el cual es una implementaci√≥n de la interface IKernel que es la que inyectaremos en nuestro c√≥digo en los constructores de las clases y tipos donde necesitemos acceder a √©ste para interactuar con las abstracciones que configuremos de Inteligencia Artificial.
‚Ä¢	En la l√≠nea 22 configuramos cu√°l ser√° el modelo que se emplee desde OpenAI para acciones de Inteligencia Artificial generativa, la t√≠pica que vemos en ChatGPT por ejemplo.
‚Ä¢	En la l√≠nea 23 configuramos cu√°l ser√° el modelo que se emplee desde OpenAI para la generaci√≥n y procesamiento de embeddings. Esto es algo que pasar√° el Semantic Kernel a quien lo requiere, principalmente la abstracci√≥n de la memoria.
‚Ä¢	En la l√≠nea 25 agregamos el ILogger al IKernel.
‚Ä¢	En la l√≠nea 26 configuramos el almac√©n de memoria, y es aqu√≠ donde obtenemos por inyecci√≥n de dependencias la instancia v√°lida de IMemoryStore que configuramos previamente como una base de datos de Qdrant.
‚Ä¢	En la l√≠nea 29, tras construir el Semantic Kernel, se lee de un directorio las funciones correspondientes las llamadas habilidades (skills) sem√°nticas. Estas son b√°sicamente, habilidades que se definen como prompts para cualquiera de los modelos de OpenAI, y de las cuales puedes encontrar m√°s informaci√≥n aqu√≠ üëâ https://learn.microsoft.com/en-us/semantic-kernel/create-plugins/ 

Fijaos que el Semantic Kernel, a diferencia de la memoria, es configurado como Scoped. Esto es para asegurarnos de que durante el ciclo de vida de una llamada estaremos usando la misma instancia del Semantic Kernel, conservando las configuraciones, pasos y encadenamientos de las abstracciones a lo largo del flujo o regla de negocio que estemos ejecutando, pero que se eliminar√° (dispondr√°) al final de √©ste.
Es muy importante entender que es un error fatal agregar el Semantic Kernel como Singleton, porque cada flujo de negocio que ejecutemos podr√≠a tener que activar diferentes componentes, abstracciones o encadenamientos y habr√≠a una afectaci√≥n entre ellos al compartirse la misma instancia, con efectos secundarios y comportamientos inesperados que son extremadamente muy dif√≠ciles de depurar. 
En ese mismo sentido, tambi√©n ser√≠a un error fatal agregar el Semantic Kernel como Transient, ya que entonces cada parte de nuestro c√≥digo que se active y tenga al Semantic Kernel como dependencia recibir√° una nueva instancia que partir√° desde cero, sin tomar en cuenta las configuraciones, componentes o abstracciones que se hayan activado o encadenado, con iguales consecuencias que el caso anterior.
Ya con esto podemos comenzar a usar el Semantic Kernel en nuestro c√≥digo tomando dependencias con la interface IKernel, la cual nos ofrecer√° todo lo necesario para interactuar con la memoria y los dem√°s componentes.
Veamos unos ejemplos de esto antes de pasar al despliegue en nuestra arquitectura 100% ¬´Cloud Native¬ª.
El c√≥digo de ejemplo busca proporcionar concejos incre√≠bles a preguntas o consultas que le realicemos a trav√©s de un API REST. Si quieres probarlo en un Postman, encontrar√°s una colecci√≥n lista para ello en el repo de GitHub en el directorio postman.
En ese sentido, lo primero es poder tener en nuestra base de datos vectorial algunos concejos base que le sirvan m√°s tarde a la Inteligencia Artificial de inspiraci√≥n para darnos una respuesta m√°s certera a nuestra consulta (yo he usado frases sacadas de aqu√≠ üëâ https://calvinrosser.com/40-pieces-of-advice/. No importa el idioma de esta entrada (en el enlace anterior est√°n en ingl√©s), ya que una de las capacidades m√°s incre√≠bles y poderosas de las tecnolog√≠as de OpenAI es que traducen el contenido en base al contexto de la entrada que reciben. As√≠, prueba el c√≥digo del repo con los concejos del enlace anterior tal cual, en ingl√©s, pero pregunt√°ndole a la IA (como ver√°s m√°s adelante) en espa√±ol o franc√©s‚Ä¶ y ver√°s que te contesta tal cual en el idioma en que has enviado tu consulta ü´¢
El siguiente c√≥digo muestra c√≥mo podemos pasar un texto al Semantic Kernel y √©ste autom√°gicamente lo almacenar√° en la memoria:

![](../../../images/numero56/cloud-native-semantic-kernel/image1c.png)

La magia ocurre en la l√≠nea 10, donde el Semantic Kernel nos abstrae de qu√© o c√≥mo funciona la memoria. Realmente lo que hace es tomar el texto que le pasemos y usando el modelo para embeddings que hemos configurado en Program.cs crear√° los embeddings del mismo y los almacenar√° en la base de datos que tambi√©n hemos definido en Program.cs, en este caso Qdrant.
Esto tiene una grand√≠sima ventaja y es que, si el d√≠a de ma√±ana queremos cambiar de base de datos vectorial, simplemente tenemos que actualizar las referencias para el contenedor de dependencias en Program.cs, y el resto de nuestro c√≥digo no se ver√° afectado.
El c√≥digo anterior no retornar√° nada en el cuerpo de la respuesta, pero en la cabecera Location nos dar√° la URL donde podremos obtener el concejo que hemos procesado. El c√≥digo es el siguiente:

![](../../../images/numero56/cloud-native-semantic-kernel/image1d.png)

Aqu√≠, en la l√≠nea 12, se usa el identificador creado en el c√≥digo del procesamiento para recuperar el contenido directamente desde la memoria. En el caso de bases de datos vectoriales, esto es recuperar el registro almacenado tal cual, no es una b√∫squeda sem√°ntica basada en vectores, sino un acceso directo por identificador al registro con la informaci√≥n.
Nuevamente podemos ver que no tenemos que preocuparnos de los embeddings, como se construyen o como se deserializan; ya de eso se encarga Semantic Kernel y la implementaci√≥n del conector de memoria.
Si el resultado de llamar al m√©todo GetAsync retorna null significa que no existe en memoria un registro con el identificador proporcionado (ergo, retornamos un HTTP 404 Not Found, como puede verse en la l√≠nea 15). En caso de que retorne un registro desde la memoria, por convenci√≥n, su valor estar√° dentro de una propiedad llamada Text que forma parte de la Metadata del registro.
Ahora bien, lo interesante es ver c√≥mo podemos usar la memoria y Semantic Kernel para realizar una b√∫squeda sem√°ntica por similitud de vectores en funci√≥n del contenido de dicha memoria (en este caso de Qdrant) para obtener un resultado generado a partir de nuestra consulta.
En el siguiente c√≥digo, podemos ver una acci√≥n del controlador que toma una pregunta y se la pasa a la memoria para obtener un resultado que pasamos luego a la Inteligencia Artificial para que genere una respuesta m√°s adaptada a nuestra consulta:

![](../../../images/numero56/cloud-native-semantic-kernel/image1e.png)

En la l√≠nea 30 se realiza la llamada a la memoria para que realice una b√∫squeda sem√°ntica por funci√≥n de similitud a trav√©s del m√©todo SearchAsync, al cual se le pasa la pregunta, la colecci√≥n donde tiene que realizar la b√∫squeda y dos par√°metros m√°s:
‚Ä¢	L√≠mite de resultados  corresponde al n√∫mero m√°ximo de resultados a obtener. Es como hacer un ¬´SELECT TOP ‚Ä¶¬ª en bases de datos relacionales.

‚Ä¢	Un √≠ndice de similitud  es un valor entre 0 y 1 (b√°sicamente un porcentaje) que determina que tan cercanos deben ser los resultados en similitud sem√°ntica para ser considerados como correctos al devolverlos. Cuanto menor sea este valor (m√°s cercano a cero) m√°s resultados obtendremos, pero probablemente con menor valor o similitud sem√°ntica; por el contrario, cuanto mayor sea el este valor (m√°s cercano a uno) existe la probabilidad de que recibamos menos resultados, incluso el no obtener resultado alguno, pero que ser√°n de mayor valor o similitud sem√°ntica.

Algo interesante es que tampoco tenemos que preocuparnos de cu√°l funci√≥n de similitud se debe emplear, nuevamente es algo que se abstrae en Semantic Kernel, y que en realidad la librer√≠a delega al conector de memoria. En el caso de Qdrant, se emplea la funci√≥n de similitud coseno, ya que es la que emplea OpenAI al generar los embeddings.
Una vez que tenemos resultados desde la memoria, se los pasamos a una habilidad (skill) de Semantic Kernel que se encargar√° de usar el modelo de completaci√≥n que hemos configurado en Program.cs para generar una respuesta m√°s apropiada a la llamada. El tema de habilidades (skilling) se sale un poquito del alcance de este art√≠culo ‚Äì que ya de por s√≠ est√° quedando bastante largo üòÖ ‚Äì pero ten√©is un ejemplo completo el repo de GitHub.
As√≠, el resultado de llamar a la habilidad es retornado como respuesta a la llamada.
Por √∫ltimo, en este apartado, recordemos que los m√©todos SaveInformationAsync, SearchAsync y GetAsync no son de Qdrant, sino de Semantic Kernel en su interface ISemanticTextMemory.
Ahora, para conectar nuestra aplicaci√≥n con la base de datos Qdrant que tenemos en Azure nos basta simplemente con desplegarla dentro de un ACA que est√© dentro del mismo ACE que la base de datos, ya que, en caso contrario, debido a que el ACA del Qdrant est√° configurado para no aceptar peticiones fuera del ACE, nuestra aplicaci√≥n no podr√° conectarse a √©sta.
Es decir, que debemos seguir los mismos pasos que seguimos para desplegar Qdrant, con dos cambios:
1.	La imagen a utilizar ser√° la de nuestra aplicaci√≥n que tendremos ‚Äì lo m√°s probable ‚Äì dentro de un Azure Container Registry (ACR) tras publicarla.
2.	Al configurar el ingreso a nuestra aplicaci√≥n (ingress) debemos apuntar como ¬´Target port¬ª al puerto 80 y en las restricciones de acceso seleccionar la opci√≥n de ¬´Accepting traffic from anywhere¬ª para poder acceder a ella desde Internet.
Finalmente tenemos que poner los valores de configuraci√≥n en nuestro Azure App Configuration. Aqu√≠ el truco para conectar el ACA de nuestra aplicaci√≥n con el ACA del Qdrant es percatarnos que el FQDN del ACA tiene el t√©rmino internal como parte de √©ste, por ejemplo:
https://aca-compartimoss-qdrant.internal.yellow-23.westeurope.azurecontainerapps.io

Muchas veces obviamos ese t√©rmino internal y no entendemos porque falla la conexi√≥n. Recordemos que hemos puesto el ACA del Qdrant para que s√≥lo acepte conexiones dentro del ACE por seguridad, ya que la versi√≥n de c√≥digo abierto para dockerizar no proporciona (por ahora) un mecanismo de seguridad, como podr√≠a ser una seguridad basada en identidades.

**A modo de conclusi√≥n**

¬°Y eso ser√≠a todo!
Podemos ver que gracias a Semantic Kernel es s√∫per sencillo integrar una abstracci√≥n de memoria para nuestras aplicaciones de Inteligencia Artificial, sean estas plug-ins o Copilots.
En el caso de Qdrant, podemos ver que es una base de datos vectorial de c√≥digo abierto (open source) bastante interesante, poderosa y muy f√°cil de desplegar e integrar de forma segura en una arquitectura 100% ¬´Cloud Native¬ª usando Azure Container Apps ‚Äì que tambi√©n son econ√≥micamente muy rentables ‚Äì y que nos dar√° la flexibilidad que buscamos a la hora de dotar de estado y permanencia a la informaci√≥n que manipulemos con tecnolog√≠as de Inteligencia Artificial, y de c√≥mo Semantic Kernel nos ofrece los medios para abstraernos de las implementaciones, con lo cual ser√° muy f√°cil pasar de una implementaci√≥n tecnol√≥gica a otra con impacto m√≠nimo sobre la base de nuestro c√≥digo.
Cualquier duda o consulta que teng√°is no dud√©is en contactarme por redes sociales, har√© todo lo posible por contestaros y ayudaros.
¬°Gracias!

**Rodrigo Liberoff** <br />
Senior Software & Cloud Architect en ENCAMINA <br />
https://www.linkedin.com/in/rliberoff  <br />
https://twitter.com/rliberoff   <br />